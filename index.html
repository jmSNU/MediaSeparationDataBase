<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MediaSeparationDatabase</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: Consolas, monospace;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        a {
            color: blue;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>MediaSeparationDatabase</h1>
    <p>This repository provides the dataset designed for training music source separation models on real-world media content. The dataset integrates multiple audio sources, enabling advanced processing for speech and music separation.</p>

    <h2>Quick Start</h2>
    <p>Run the following command to generate the dataset using the default configuration:</p>
    <pre><code>python3 gen_dataset.py --config config.yaml</code></pre>

    <h2>Overview of Dataset Generation</h2>
    <p>The dataset is constructed through a multi-step process, as illustrated below:</p>
    <img src="./figures/overview.png" alt="Overview of Dataset Generation">

    <h2>STT Result for Media Source Separation</h2>
    <p>The following visualization highlights the Speech-to-Text (STT) evaluation results, demonstrating the effectiveness of our approach:</p>
    <img src="./figures/stt.png" alt="STT Result for Media Source Separation">

    <h2>Demonstration</h2>
    <p>We provide a demonstration of separation results produced by HT Demucs trained on this dataset.</p>
    <h3>Sample 1</h3>
    <p><img src="path/to/your_sample1_image.png" alt="Original Source"></p>
    <h3>Sample 2</h3>
    <p><img src="path/to/your_sample2_image.png" alt="Separated Sample"></p>

    <h2>Workspace Structure</h2>
    <p>The following directory structure outlines the organization of raw and processed data:</p>
    <pre><code>data/
├── cv-corpus-17-curated         # Common Voice curated dataset
│   ├── common_voice_curated_train
│   │   ├── common_voice_en_12345.wav
│   │   ├── common_voice_en_67890.wav
│   │   └── ...
│   └── common_voice_curated_test
│       ├── common_voice_en_2737.wav
│       ├── common_voice_en_71783.wav
│       └── ...
├── musdb                       # Music stem dataset
│   ├── train
│   │   ├── music-001.stem.mp4
│   │   ├── music-002.stem.mp4
│   │   └── ...
│   └── test
│       ├── music-003.stem.mp4
│       ├── music-004.stem.mp4
│       └── ...
└── musan                       # Noise dataset
    ├── music                   # Not used here
    │   ├── music-001.wav
    │   ├── music-002.wav
    │   └── ...
    ├── noise                   # Background noise sound dataset
    │   ├── free-sound          # Free Sound source
    │   │   ├── noise-sound-001.wav
    │   │   ├── noise-sound-002.wav
    │   │   └── ...
    │   └── sound-bible         # Sound Bible source
    │       ├── noise-sound-bible-0071.wav
    │       ├── noise-sound-bible-0072.wav
    │       └── ...
    └── speech                  # Not used here
        ├── speech-001.wav
        ├── speech-002.wav
        └── ...
</code></pre>

    <h2>Example of Dataset Generated</h2>
    <p>The processed dataset is organized as follows:</p>
    <pre><code>processed_data_curated/
├── train/
│   ├── Actions - Devil's Words_mix.wav
│   ├── Actions - Devil's Words_music.wav
│   ├── Actions - Devil's Words_speech.wav
│   ├── Actions - Devil's Words_others.wav
│   ├── ...
├── validation/
│   ├── AvaLuna - Waterduct_mix.wav
│   ├── AvaLuna - Waterduct_music.wav
│   ├── AvaLuna - Waterduct_speech.wav
│   ├── AvaLuna - Waterduct_others.wav
│   ├── ...
├── test/
│   ├── Arise - Run Run Run_mix.wav
│   ├── Arise - Run Run Run_music.wav
│   ├── Arise - Run Run Run_speech.wav
│   ├── Arise - Run Run Run_others.wav
│   ├── ...
├── train.csv
├── validation.csv
└── test.csv
</code></pre>

    <h2>Download Datasets</h2>
    <ul>
        <li><a href="https://drive.google.com/file/d/15QMdtI17JFjKzPLIVEMZDBJMJef7PJsx/view?usp=sharing">Download MUSDB</a></li>
        <li><a href="https://drive.google.com/file/d/1r-rqnSzligtNrYloBX4hCl7lkCR12ZQ1/view?usp=sharing">Download MUSAN Noise</a></li>
        <li><a href="https://drive.google.com/file/d/1E2tcYXM7e3HgUGVa7oH2ntoG0-VcQR9o/view?usp=sharing">Download our preprocessed curated Dataset</a></li>
    </ul>

    <h2>Citation</h2>
    <p>If you use this dataset in your work, please cite as follows:</p>
    <pre><code>@misc{lee2024towards,
  title        = {TOWARDS REAL-WORLD MEDIA SEPARATION: ENHANCING MUSDB18 WITH OPEN-SOURCE SPEECH DATA FOR SPEECH AND MUSIC PROCESSING},
  author       = {Jung Min Lee and Su Min Park},
  year         = {2024},
  note         = {*Equal contribution},
  howpublished = {\url{https://github.com/jmSNU/MediaSeparataionDatabase.git}},
}</code></pre>

    <h2>Acknowledgements</h2>
    <p>Our dataset is built using the following sources:</p>
    <ul>
        <li><a href="https://commonvoice.mozilla.org/en/datasets">Common Voice v17.0</a></li>
        <li><a href="https://sigsep.github.io/datasets/musdb.html">MUSDB18</a></li>
        <li><a href="http://www.openslr.org/17/">MUSAN</a></li>
    </ul>
    <p>[2024.12.19] Thanks to the contributors of these datasets for providing valuable resources for audio processing research.</p>
</body>
</html>
